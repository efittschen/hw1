{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36b8b334",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/efittsc1/projects/hyperparameter_experiments/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from torch.profiler import profile, ProfilerActivity, record_function\n",
    "from torch.utils.flop_counter import FlopCounterMode\n",
    "import psutil\n",
    "import pandas as pd\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments, DataCollatorWithPadding, TrainerCallback\n",
    "import random\n",
    "import datasets\n",
    "import wandb\n",
    "import numpy as np\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b420247",
   "metadata": {},
   "source": [
    "### Exercise 4.1\n",
    "\n",
    "Build a classifier based on ModernBERT and fine-tune the classification head only (not the model weights) so that\n",
    "the accuracy is maximized for this task. Plot the accuracy on train and dev (validation) sets over the course of\n",
    "training. Report the results on the test set corresponding to your best model measured on the dev (validation) set\n",
    "in Table 1. Include the results in Table 1. Include a link to your code on Github."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "396f55e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ModernBertForSequenceClassification were not initialized from the model checkpoint at answerdotai/ModernBERT-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        \"answerdotai/ModernBERT-base\", num_labels=2\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45261752",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ModernBertForSequenceClassification were not initialized from the model checkpoint at answerdotai/ModernBERT-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"answerdotai/ModernBERT-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "30d1ef6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "strategy_qa = datasets.load_dataset(\"wics/strategy-qa\", split=\"test\")\n",
    "ds = strategy_qa.train_test_split(test_size=0.2, seed=42, shuffle=True)\n",
    "tv = ds[\"test\"].train_test_split(test_size=0.5, seed=42, shuffle=True)\n",
    "label_map = {\"true\": 1, \"false\": 0}\n",
    "\n",
    "ds = datasets.DatasetDict({\n",
    "    \"train\": ds[\"train\"],\n",
    "    \"test\": tv[\"test\"],\n",
    "    \"validation\": tv[\"train\"]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4a903fd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 1832\n",
      "Validation size: 229\n",
      "Test size: 229\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train size: {len(ds['train'])}\")\n",
    "print(f\"Validation size: {len(ds['validation'])}\")\n",
    "print(f\"Test size: {len(ds['test'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77636ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(ex):\n",
    "    ans = ex[\"answer\"]\n",
    "    y = int(bool(ans))\n",
    "    # appending facts to the question, because the model is not doing well at all\n",
    "    text = ex[\"question\"]\n",
    "    text = \" \".join(ex[\"facts\"]) + \" \" + ex[\"question\"]\n",
    "    enc = tokenizer(text)\n",
    "    enc[\"labels\"] = y\n",
    "    return enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d0b8f4dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/1832 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1832/1832 [00:00<00:00, 3128.94 examples/s]\n",
      "Map: 100%|██████████| 229/229 [00:00<00:00, 3821.42 examples/s]\n",
      "Map: 100%|██████████| 229/229 [00:00<00:00, 3930.98 examples/s]\n"
     ]
    }
   ],
   "source": [
    "ds = ds.map(preprocess, remove_columns=strategy_qa.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "945409b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True labels in training set: 854\n",
      "False labels in training set: 978\n"
     ]
    }
   ],
   "source": [
    "print(f\"True labels in training set: {sum(ds['train']['labels'])}\")\n",
    "print(f\"False labels in training set: {len(ds['train']) - sum(ds['train']['labels'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0a04a551",
   "metadata": {},
   "outputs": [],
   "source": [
    "collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "72a41e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = evaluate.load(\"accuracy\")\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return accuracy.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c4792984",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainEvalCallback(TrainerCallback):\n",
    "    def __init__(self, trainer, sample_size=229):\n",
    "        self.trainer = trainer\n",
    "        self.sample_size = sample_size\n",
    "        self.train_sample = None\n",
    "\n",
    "    def on_train_begin(self, args, state, control, **kwargs):\n",
    "        ds = self.trainer.train_dataset\n",
    "        self.train_sample = ds.select(range(self.sample_size))\n",
    "\n",
    "    def on_evaluate(self, args, state, control, **kwargs):\n",
    "        metrics = self.trainer.predict(self.train_sample).metrics\n",
    "        self.trainer.log(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "97ffe4e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1221447/2303311450.py:35: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "Some weights of ModernBertForSequenceClassification were not initialized from the model checkpoint at answerdotai/ModernBERT-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total trainable parameters: 1538\n"
     ]
    }
   ],
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir=\"modernbert-strategyqa\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    logging_strategy=\"steps\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_steps=10,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    learning_rate= 2e-5,\n",
    "    warmup_ratio=0.2,\n",
    "    greater_is_better=True,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0,\n",
    "    fp16=True,\n",
    "    report_to=[\"wandb\"],\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "def model_init():\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        \"answerdotai/ModernBERT-base\", num_labels=2\n",
    "    )\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "    for name, param in model.named_parameters():\n",
    "        if \"classifier\" in name:\n",
    "            param.requires_grad = True\n",
    "    total_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"Total trainable parameters: {total_trainable_params}\")\n",
    "    return model\n",
    "\n",
    "trainer = Trainer(\n",
    "    model_init=model_init,\n",
    "    args=args,\n",
    "    train_dataset=ds[\"train\"],\n",
    "    eval_dataset=ds[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.add_callback(TrainEvalCallback(trainer))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e2b8278",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"def hp_space(trial):\n",
    "    return {\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 5e-6, 5e-3, log=True),\n",
    "    }\n",
    "\n",
    "best = trainer.hyperparameter_search(\n",
    "    backend=\"optuna\",\n",
    "    direction=\"maximize\",\n",
    "    n_trials=25,\n",
    "    hp_space=hp_space,\n",
    "    compute_objective=lambda m: m[\"eval_accuracy\"],\n",
    ")\n",
    "print(best)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2236dd73",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.args.learning_rate = 0.002371420117372919"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a1f063ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': None, 'bos_token_id': None}.\n",
      "Some weights of ModernBertForSequenceClassification were not initialized from the model checkpoint at answerdotai/ModernBERT-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total trainable parameters: 1538\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/efittsc1/projects/hyperparameter_experiments/.venv/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='174' max='174' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [174/174 00:30, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.733000</td>\n",
       "      <td>0.727907</td>\n",
       "      <td>0.510917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.712400</td>\n",
       "      <td>0.701448</td>\n",
       "      <td>0.576419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.626400</td>\n",
       "      <td>0.704988</td>\n",
       "      <td>0.598253</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8' max='8' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [8/8 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/efittsc1/projects/hyperparameter_experiments/.venv/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/efittsc1/projects/hyperparameter_experiments/.venv/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/efittsc1/projects/hyperparameter_experiments/.venv/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/efittsc1/projects/hyperparameter_experiments/.venv/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/efittsc1/projects/hyperparameter_experiments/.venv/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/efittsc1/projects/hyperparameter_experiments/.venv/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/efittsc1/projects/hyperparameter_experiments/.venv/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7049884796142578, 'eval_accuracy': 0.5982532751091703, 'eval_runtime': 1.0863, 'eval_samples_per_second': 210.801, 'eval_steps_per_second': 7.364, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "trainer.train()\n",
    "print(trainer.evaluate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cb84be9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/efittsc1/projects/hyperparameter_experiments/.venv/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/efittsc1/projects/hyperparameter_experiments/.venv/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7049884796142578, 'eval_accuracy': 0.5982532751091703, 'eval_runtime': 1.0953, 'eval_samples_per_second': 209.082, 'eval_steps_per_second': 7.304, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/efittsc1/projects/hyperparameter_experiments/.venv/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/efittsc1/projects/hyperparameter_experiments/.venv/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7125810384750366, 'eval_accuracy': 0.5240174672489083, 'eval_runtime': 1.0842, 'eval_samples_per_second': 211.209, 'eval_steps_per_second': 7.378, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "print(trainer.evaluate(ds[\"validation\"]))\n",
    "print(trainer.evaluate(ds[\"test\"]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
